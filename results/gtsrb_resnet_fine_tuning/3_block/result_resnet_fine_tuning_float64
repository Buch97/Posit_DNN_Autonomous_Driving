Float64 mode
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_4 (InputLayer)         [(None, 32, 32, 3)]       0
_________________________________________________________________
sequential_1 (Sequential)    (None, 32, 32, 3)         0
_________________________________________________________________
tf.math.truediv_1 (TFOpLambd (None, 32, 32, 3)         0
_________________________________________________________________
tf.math.subtract_1 (TFOpLamb (None, 32, 32, 3)         0
_________________________________________________________________
resnet50v2 (Functional)      (None, 1, 1, 2048)        23564800
_________________________________________________________________
my_glo_avg_pool (GlobalAvera (None, 2048)              0
_________________________________________________________________
predictions (Dense)          (None, 43)                88107
=================================================================
Total params: 23,652,907
Trainable params: 15,038,507
Non-trainable params: 8,614,400
_________________________________________________________________
DATASET: gtsrb
WARNING:tensorflow:From /home/matteo/PycharmProjects/seai_project/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5059: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.
Instructions for updating:
The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.
2023-09-07 10:16:17.177959: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)
accuracy: 0.759245090538128
123/123 [==============================] - 9s 61ms/step - loss: 0.7413 - accuracy: 0.7592
Test accuracy: 0.759, test loss: 0.741
Classification report:
              precision    recall  f1-score   support

           0     0.9286    0.6190    0.7429        21
           1     0.6444    0.8243    0.7233       222
           2     0.8011    0.6622    0.7251       225
           3     0.6800    0.6028    0.6391       141
           4     0.5498    0.6414    0.5921       198
           5     0.6549    0.5000    0.5671       186
           6     0.6415    0.8095    0.7158        42
           7     0.6892    0.7083    0.6986       144
           8     0.5726    0.5035    0.5358       141
           9     0.8931    0.7959    0.8417       147
          10     0.7277    0.8109    0.7671       201
          11     0.5500    0.8333    0.6627       132
          12     0.8991    0.9333    0.9159       210
          13     0.9952    0.9583    0.9764       216
          14     0.9610    0.9487    0.9548        78
          15     0.7647    0.8254    0.7939        63
          16     0.9286    0.9286    0.9286        42
          17     0.9820    0.9820    0.9820       111
          18     0.6803    0.8333    0.7491       120
          19     0.8125    0.6190    0.7027        21
          20     0.5517    0.4444    0.4923        36
          21     0.7407    0.6061    0.6667        33
          22     0.7368    0.7179    0.7273        39
          23     0.7742    0.4706    0.5854        51
          24     0.8000    0.4444    0.5714        27
          25     0.6615    0.8467    0.7427       150
          26     0.7321    0.6833    0.7069        60
          27     0.8333    0.4167    0.5556        24
          28     0.7500    0.3889    0.5122        54
          29     0.9167    0.4074    0.5641        27
          30     0.7179    0.6222    0.6667        45
          31     0.8788    0.7436    0.8056        78
          32     0.9167    0.4583    0.6111        24
          33     0.8378    0.8986    0.8671        69
          34     0.8636    0.9048    0.8837        42
          35     0.8800    0.9167    0.8980       120
          36     0.8250    0.8462    0.8354        39
          37     0.9000    0.8571    0.8780        21
          38     0.8829    0.8744    0.8786       207
          39     1.0000    0.8667    0.9286        30
          40     0.6889    0.8611    0.7654        36
          41     1.0000    0.5833    0.7368        24
          42     0.9091    0.8333    0.8696        24

    accuracy                         0.7592      3921
   macro avg     0.7943    0.7217    0.7433      3921
weighted avg     0.7701    0.7592    0.7566      3921