{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattemugno/seai_project/blob/master/GTSDB_object_detection_retinanet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqeLdZUc-rOm"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j12DjLpsNts5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a8e4d0-ba45-493b-a776-38eec7ed3ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2P369K5R8sHZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import LayerNormalization, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lzhPihmX87hd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 43\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NP4XSZ0qusnK"
      },
      "outputs": [],
      "source": [
        "class_ids = {\n",
        "    'speed_limit_20': 1,\n",
        "    'speed_limit_30': 2,\n",
        "    'speed_limit_50': 3,\n",
        "    'speed_limit_60': 4,\n",
        "    'speed_limit_70': 5,\n",
        "    'speed_limit_80': 6,\n",
        "    'restriction_ends_80': 7,\n",
        "    'speed_limit_100': 8,\n",
        "    'speed_limit_120': 9,\n",
        "    'no_overtaking': 10,\n",
        "    'no_overtaking_trucks': 11,\n",
        "    'priority_at_next_intersection': 12,\n",
        "    'priority_road': 13,\n",
        "    'give_way': 14,\n",
        "    'stop': 15,\n",
        "    'no_traffic_both_ways': 16,\n",
        "    'no_trucks': 17,\n",
        "    'no_entry': 18,\n",
        "    'danger': 19,\n",
        "    'bend_left': 20,\n",
        "    'bend_right': 21,\n",
        "    'bend': 22,\n",
        "    'uneven_road': 23,\n",
        "    'slippery_road': 24,\n",
        "    'road_narrows': 25,\n",
        "    'construction': 26,\n",
        "    'traffic_signal': 27,\n",
        "    'pedestrian_crossing': 28,\n",
        "    'school_crossing': 29,\n",
        "    'cycles_crossing': 30,\n",
        "    'snow': 31,\n",
        "    'animals': 32,\n",
        "    'restriction_ends': 33,\n",
        "    'go_right': 34,\n",
        "    'go_left': 35,\n",
        "    'go_straight': 36,\n",
        "    'go_right_or_straight': 37,\n",
        "    'go_left_or_straight': 38,\n",
        "    'keep_right': 39,\n",
        "    'keep_left': 40,\n",
        "    'roundabout': 41,\n",
        "    'restriction_ends_overtaking': 42,\n",
        "    'restriction_ends_overtaking_trucks': 43\n",
        "}\n",
        "\n",
        "keys_array = list(class_ids.keys())\n",
        "class_mapping = dict(zip(range(1, len(class_ids) + 1), class_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_clone_function(layer):\n",
        "    if isinstance(layer, Model):\n",
        "        return clone_model(layer, clone_function=my_clone_function)\n",
        "    if isinstance(layer, BatchNormalization) or isinstance(layer, LayerNormalization):\n",
        "        config = layer.get_config()\n",
        "        return layer.__class__.from_config(config)\n",
        "    if isinstance(layer, Layer):\n",
        "        config = layer.get_config()\n",
        "        config['dtype'] = 'float16'\n",
        "        return layer.__class__.from_config(config)"
      ],
      "metadata": {
        "id": "-hc0ZCz0svp8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPVJVRDCocW"
      },
      "source": [
        "#RetinaNet implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8hGBoFv3CuEY"
      },
      "outputs": [],
      "source": [
        "def swap_xy(boxes):\n",
        "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "      swapped boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    \"\"\"Changes the box format to center, width and height.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[xmin, ymin, xmax, ymax]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "    \"\"\"Changes the box format to corner coordinates\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zwUKebxnCw2H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
        "\n",
        "    Arguments:\n",
        "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
        "        jth column holds the IOU between ith box and jth box from\n",
        "        boxes1 and boxes2 respectively.\n",
        "    \"\"\"\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QShklLxGC4IW"
      },
      "outputs": [],
      "source": [
        "class AnchorBox:\n",
        "    \"\"\"Generates anchor boxes.\n",
        "\n",
        "    This class has operations to generate anchor boxes for feature maps at\n",
        "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
        "    format `[x, y, width, height]`.\n",
        "\n",
        "    Attributes:\n",
        "      aspect_ratios: A list of float values representing the aspect ratios of\n",
        "        the anchor boxes at each location on the feature map\n",
        "      scales: A list of float values representing the scale of the anchor boxes\n",
        "        at each location on the feature map.\n",
        "      num_anchors: The number of anchor boxes at each location on feature map\n",
        "      areas: A list of float values representing the areas of the anchor\n",
        "        boxes for each feature map in the feature pyramid.\n",
        "      strides: A list of float value representing the strides for each feature\n",
        "        map in the feature pyramid.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
        "        of the feature pyramid.\n",
        "        \"\"\"\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
        "\n",
        "        Arguments:\n",
        "          feature_height: An integer representing the height of the feature map.\n",
        "          feature_width: An integer representing the width of the feature map.\n",
        "          level: An integer representing the level of the feature map in the\n",
        "            feature pyramid.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes with the shape\n",
        "          `(feature_height * feature_width * num_anchors, 4)`\n",
        "        \"\"\"\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
        "\n",
        "        Arguments:\n",
        "          image_height: Height of the input image.\n",
        "          image_width: Width of the input image.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes for all the feature maps, stacked as a single tensor\n",
        "            with shape `(total_anchors, 4)`\n",
        "        \"\"\"\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "9v5SumWhC6uk"
      },
      "outputs": [],
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
        "        having normalized coordinates.\n",
        "\n",
        "    Returns:\n",
        "      Randomly flipped image and boxes\n",
        "    \"\"\"\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        boxes = tf.stack(\n",
        "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "    return image, boxes\n",
        "\n",
        "\n",
        "def resize_and_pad_image(\n",
        "    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "):\n",
        "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
        "\n",
        "    1. Resizes images so that the shorter side is equal to `min_side`\n",
        "    2. If the longer side is greater than `max_side`, then resize the image\n",
        "      with longer side equal to `max_side`\n",
        "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
        "    `stride`\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      min_side: The shorter side of the image is resized to this value, if\n",
        "        `jitter` is set to None.\n",
        "      max_side: If the longer side of the image exceeds this value after\n",
        "        resizing, the image is resized such that the longer side now equals to\n",
        "        this value.\n",
        "      jitter: A list of floats containing minimum and maximum size for scale\n",
        "        jittering. If available, the shorter side of the image will be\n",
        "        resized to a random value in this range.\n",
        "      stride: The stride of the smallest feature map in the feature pyramid.\n",
        "        Can be calculated using `image_size / feature_map_size`.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image.\n",
        "      image_shape: Shape of the image before padding.\n",
        "      ratio: The scaling factor used to resize the image\n",
        "    \"\"\"\n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if jitter is not None:\n",
        "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "    min_side *= 0.5\n",
        "    max_side *= 0.5\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio\n",
        "\n",
        "\n",
        "def preprocess_data(sample):\n",
        "    \"\"\"Applies preprocessing step to a single sample\n",
        "\n",
        "    Arguments:\n",
        "      sample: A dict representing a single training sample.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image with random horizontal flipping applied.\n",
        "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
        "        of the format `[x, y, width, height]`.\n",
        "      class_id: An tensor representing the class id of the objects, having\n",
        "        shape `(num_objects,)`.\n",
        "    \"\"\"\n",
        "    image = sample[\"image\"]\n",
        "    #bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    bbox = sample[\"objects\"][\"bbox\"]\n",
        "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lEXLu7epRY9S"
      },
      "outputs": [],
      "source": [
        "class LabelEncoder:\n",
        "    \"\"\"Transforms the raw labels into targets for training.\n",
        "\n",
        "    This class has operations to generate targets for a batch of samples which\n",
        "    is made up of the input images, bounding boxes for the objects present and\n",
        "    their class ids.\n",
        "\n",
        "    Attributes:\n",
        "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
        "      box_variance: The scaling factors used to scale the bounding box targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
        "\n",
        "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
        "          to get a `(M, N)` shaped matrix.\n",
        "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
        "          the anchor box provided the IOU is greater than `match_iou`.\n",
        "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
        "          box is assigned with the background class.\n",
        "        4. The remaining anchor boxes that do not have any class assigned are\n",
        "          ignored during training.\n",
        "\n",
        "        Arguments:\n",
        "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
        "            representing all the anchor boxes for a given input image shape,\n",
        "            where each anchor box is of the format `[x, y, width, height]`.\n",
        "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
        "            the ground truth boxes, where each box is of the format\n",
        "            `[x, y, width, height]`.\n",
        "          match_iou: A float value representing the minimum IOU threshold for\n",
        "            determining if a ground truth box can be assigned to an anchor box.\n",
        "          ignore_iou: A float value representing the IOU threshold under which\n",
        "            an anchor box is assigned to the background class.\n",
        "\n",
        "        Returns:\n",
        "          matched_gt_idx: Index of the matched object\n",
        "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
        "            truth boxes.\n",
        "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
        "            training\n",
        "        \"\"\"\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HWVaqZpSC-p2"
      },
      "outputs": [],
      "source": [
        "def get_backbone():\n",
        "    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n",
        "    backbone = keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )\n",
        "\n",
        "class FeaturePyramid(keras.layers.Layer):\n",
        "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"FeaturePyramid\", **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
        "\n",
        "def build_head(output_filters, bias_init):\n",
        "    \"\"\"Builds the class/box predictions head.\n",
        "\n",
        "    Arguments:\n",
        "      output_filters: Number of convolution filters in the final layer.\n",
        "      bias_init: Bias Initializer for the final convolution layer.\n",
        "\n",
        "    Returns:\n",
        "      A keras sequential model representing either the classification\n",
        "        or the box regression head depending on `output_filters`.\n",
        "    \"\"\"\n",
        "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
        "        )\n",
        "        head.add(keras.layers.ReLU())\n",
        "    head.add(\n",
        "        keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5fGKdFNaDFG0"
      },
      "outputs": [],
      "source": [
        "class RetinaNet(keras.Model):\n",
        "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"RetinaNet\", **kwargs)\n",
        "        self.fpn = FeaturePyramid(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IfkeWF_UDSSq"
      },
      "outputs": [],
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset\n",
        "      confidence_threshold: Minimum class probability, below which detections\n",
        "        are pruned.\n",
        "      nms_iou_threshold: IOU threshold for the NMS operation\n",
        "      max_detections_per_class: Maximum number of detections to retain per\n",
        "       class.\n",
        "      max_detections: Maximum number of detections to retain across all\n",
        "        classes.\n",
        "      box_variance: The scaling factors used to scale the bounding box\n",
        "        predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=43,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vokB_UykDUm4"
      },
      "outputs": [],
      "source": [
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Focal loss\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=43, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super().__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om8SlKsPRECz"
      },
      "source": [
        "#RetinaNet float16 implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "53N43mB5REC7"
      },
      "outputs": [],
      "source": [
        "def swap_xy(boxes):\n",
        "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "      swapped boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    \"\"\"Changes the box format to center, width and height.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[xmin, ymin, xmax, ymax]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "    \"\"\"Changes the box format to corner coordinates\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3iQhfIRwREC8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
        "\n",
        "    Arguments:\n",
        "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
        "        jth column holds the IOU between ith box and jth box from\n",
        "        boxes1 and boxes2 respectively.\n",
        "    \"\"\"\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yoBBQZXgREC8"
      },
      "outputs": [],
      "source": [
        "class AnchorBox:\n",
        "    \"\"\"Generates anchor boxes.\n",
        "\n",
        "    This class has operations to generate anchor boxes for feature maps at\n",
        "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
        "    format `[x, y, width, height]`.\n",
        "\n",
        "    Attributes:\n",
        "      aspect_ratios: A list of float values representing the aspect ratios of\n",
        "        the anchor boxes at each location on the feature map\n",
        "      scales: A list of float values representing the scale of the anchor boxes\n",
        "        at each location on the feature map.\n",
        "      num_anchors: The number of anchor boxes at each location on feature map\n",
        "      areas: A list of float values representing the areas of the anchor\n",
        "        boxes for each feature map in the feature pyramid.\n",
        "      strides: A list of float value representing the strides for each feature\n",
        "        map in the feature pyramid.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
        "        of the feature pyramid.\n",
        "        \"\"\"\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
        "\n",
        "        Arguments:\n",
        "          feature_height: An integer representing the height of the feature map.\n",
        "          feature_width: An integer representing the width of the feature map.\n",
        "          level: An integer representing the level of the feature map in the\n",
        "            feature pyramid.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes with the shape\n",
        "          `(feature_height * feature_width * num_anchors, 4)`\n",
        "        \"\"\"\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
        "\n",
        "        Arguments:\n",
        "          image_height: Height of the input image.\n",
        "          image_width: Width of the input image.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes for all the feature maps, stacked as a single tensor\n",
        "            with shape `(total_anchors, 4)`\n",
        "        \"\"\"\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "XxU2RSpOREC8"
      },
      "outputs": [],
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
        "        having normalized coordinates.\n",
        "\n",
        "    Returns:\n",
        "      Randomly flipped image and boxes\n",
        "    \"\"\"\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        boxes = tf.stack(\n",
        "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "    return image, boxes\n",
        "\n",
        "\n",
        "def resize_and_pad_image_float16(\n",
        "    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "):\n",
        "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
        "\n",
        "    1. Resizes images so that the shorter side is equal to `min_side`\n",
        "    2. If the longer side is greater than `max_side`, then resize the image\n",
        "      with longer side equal to `max_side`\n",
        "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
        "    `stride`\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      min_side: The shorter side of the image is resized to this value, if\n",
        "        `jitter` is set to None.\n",
        "      max_side: If the longer side of the image exceeds this value after\n",
        "        resizing, the image is resized such that the longer side now equals to\n",
        "        this value.\n",
        "      jitter: A list of floats containing minimum and maximum size for scale\n",
        "        jittering. If available, the shorter side of the image will be\n",
        "        resized to a random value in this range.\n",
        "      stride: The stride of the smallest feature map in the feature pyramid.\n",
        "        Can be calculated using `image_size / feature_map_size`.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image.\n",
        "      image_shape: Shape of the image before padding.\n",
        "      ratio: The scaling factor used to resize the image\n",
        "    \"\"\"\n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if jitter is not None:\n",
        "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "    min_side *= 0.5\n",
        "    max_side *= 0.5\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio\n",
        "\n",
        "\n",
        "def preprocess_data(sample):\n",
        "    \"\"\"Applies preprocessing step to a single sample\n",
        "\n",
        "    Arguments:\n",
        "      sample: A dict representing a single training sample.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image with random horizontal flipping applied.\n",
        "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
        "        of the format `[x, y, width, height]`.\n",
        "      class_id: An tensor representing the class id of the objects, having\n",
        "        shape `(num_objects,)`.\n",
        "    \"\"\"\n",
        "    image = sample[\"image\"]\n",
        "    #bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    bbox = sample[\"objects\"][\"bbox\"]\n",
        "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7bqfjmO7REC8"
      },
      "outputs": [],
      "source": [
        "class LabelEncoder:\n",
        "    \"\"\"Transforms the raw labels into targets for training.\n",
        "\n",
        "    This class has operations to generate targets for a batch of samples which\n",
        "    is made up of the input images, bounding boxes for the objects present and\n",
        "    their class ids.\n",
        "\n",
        "    Attributes:\n",
        "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
        "      box_variance: The scaling factors used to scale the bounding box targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
        "\n",
        "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
        "          to get a `(M, N)` shaped matrix.\n",
        "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
        "          the anchor box provided the IOU is greater than `match_iou`.\n",
        "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
        "          box is assigned with the background class.\n",
        "        4. The remaining anchor boxes that do not have any class assigned are\n",
        "          ignored during training.\n",
        "\n",
        "        Arguments:\n",
        "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
        "            representing all the anchor boxes for a given input image shape,\n",
        "            where each anchor box is of the format `[x, y, width, height]`.\n",
        "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
        "            the ground truth boxes, where each box is of the format\n",
        "            `[x, y, width, height]`.\n",
        "          match_iou: A float value representing the minimum IOU threshold for\n",
        "            determining if a ground truth box can be assigned to an anchor box.\n",
        "          ignore_iou: A float value representing the IOU threshold under which\n",
        "            an anchor box is assigned to the background class.\n",
        "\n",
        "        Returns:\n",
        "          matched_gt_idx: Index of the matched object\n",
        "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
        "            truth boxes.\n",
        "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
        "            training\n",
        "        \"\"\"\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SZLcsRMyREC9"
      },
      "outputs": [],
      "source": [
        "def get_backbone_float16():\n",
        "    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n",
        "    float16_backbone = keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "\n",
        "    backbone = clone_model(float16_backbone, clone_function=my_clone_function)\n",
        "\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )\n",
        "\n",
        "class FeaturePyramidFloat16(keras.layers.Layer):\n",
        "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"FeaturePyramidFloat16\", **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\", dtype='float16')\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\", dtype='float16')\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\", dtype='float16')\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\", dtype='float16')\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\", dtype='float16')\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\", dtype='float16')\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\", dtype='float16')\n",
        "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\", dtype='float16')\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2, dtype='float16')\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
        "\n",
        "def build_head_float16(output_filters, bias_init):\n",
        "    \"\"\"Builds the class/box predictions head.\n",
        "\n",
        "    Arguments:\n",
        "      output_filters: Number of convolution filters in the final layer.\n",
        "      bias_init: Bias Initializer for the final convolution layer.\n",
        "\n",
        "    Returns:\n",
        "      A keras sequential model representing either the classification\n",
        "        or the box regression head depending on `output_filters`.\n",
        "    \"\"\"\n",
        "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init, dtype='float16')\n",
        "        )\n",
        "        head.add(keras.layers.ReLU(dtype='float16'))\n",
        "    head.add(\n",
        "        keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "            dtype='float16'\n",
        "        )\n",
        "    )\n",
        "    return head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AiECGhphREC9"
      },
      "outputs": [],
      "source": [
        "class RetinaNetFloat16(keras.Model):\n",
        "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"RetinaNetFloat16\", **kwargs)\n",
        "        self.fpn = FeaturePyramidFloat16(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head_float16(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head_float16(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3o16m9_yREC9"
      },
      "outputs": [],
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset\n",
        "      confidence_threshold: Minimum class probability, below which detections\n",
        "        are pruned.\n",
        "      nms_iou_threshold: IOU threshold for the NMS operation\n",
        "      max_detections_per_class: Maximum number of detections to retain per\n",
        "       class.\n",
        "      max_detections: Maximum number of detections to retain across all\n",
        "        classes.\n",
        "      box_variance: The scaling factors used to scale the bounding box\n",
        "        predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=43,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        box_predictions = tf.cast(box_predictions, tf.float32)\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            tf.cast(cls_predictions, tf.float32),\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VMhzK6l7REC-"
      },
      "outputs": [],
      "source": [
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Focal loss\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=43, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super().__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoK6gNCx-vDa"
      },
      "source": [
        "#Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rKZEPJAs_zpc"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/class/label': tf.io.VarLenFeature(tf.int64)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    image = tf.image.decode_image(example['image/encoded'])\n",
        "    image.set_shape((None, None, 3))\n",
        "\n",
        "    xmin = example['image/object/bbox/xmin'].values\n",
        "    ymin = example['image/object/bbox/ymin'].values\n",
        "    xmax = example['image/object/bbox/xmax'].values\n",
        "    ymax = example['image/object/bbox/ymax'].values\n",
        "    class_id = example['image/object/class/label'].values\n",
        "\n",
        "    bounding_box = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n",
        "    area = (xmax - xmin) * (ymax - ymin)\n",
        "\n",
        "    objects = {\n",
        "        'area': tf.cast(area, dtype=tf.float32),\n",
        "        'bbox': tf.cast(bounding_box, dtype=tf.float32),\n",
        "        'id': tf.cast(class_id, dtype=tf.float32),\n",
        "        'is_crowd': False,\n",
        "        'label': tf.cast(class_id, dtype=tf.float32),\n",
        "    }\n",
        "\n",
        "    output_dict = {\n",
        "        'image': image,\n",
        "        'image/filename': example['image/filename'],\n",
        "        'image/source_id': example['image/source_id'],\n",
        "        'objects': objects,\n",
        "    }\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "train_ds = tf.data.TFRecordDataset('/content/drive/Shareddrives/seai_project/train.record')\n",
        "train_ds = train_ds.map(parse_tfrecord)\n",
        "\n",
        "eval_ds = tf.data.TFRecordDataset('/content/drive/Shareddrives/seai_project/val.record')\n",
        "eval_ds = eval_ds.map(parse_tfrecord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kczIbubjARZ7"
      },
      "source": [
        "#Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "_u0TspU9nwMQ"
      },
      "outputs": [],
      "source": [
        "model_dir = '/content/drive/Shareddrives/seai_project/models/prova'\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "-cv7v1PeDZxb"
      },
      "outputs": [],
      "source": [
        "resnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(NUM_CLASSES)\n",
        "model = RetinaNet(NUM_CLASSES, resnet50_backbone)\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "WvolCW9xmyOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34651407-f848-47ff-8bcf-ef62c57d0c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"RetinaNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " FeaturePyramid (FeaturePyr  multiple                  31585152  \n",
            " amid)                                                           \n",
            "                                                                 \n",
            " sequential_62 (Sequential)  (None, None, None, 387)   3252355   \n",
            "                                                                 \n",
            " sequential_63 (Sequential)  (None, None, None, 36)    2443300   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37280807 (142.21 MB)\n",
            "Trainable params: 37227687 (142.01 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = (None, None, None, 3)\n",
        "model.build(input_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxMr0BXtlq_"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bz1IddWEAW_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "callbacks_list = [\n",
        "      #keras.callbacks.EarlyStopping(\n",
        "      #monitor='val_loss',\n",
        "      #min_delta=1e-3,\n",
        "      #patience=5,),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only = True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPPX5KhNIaxV",
        "outputId": "d1080639-465f-449a-944c-09a4e623375b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-e7eadf0975dc>:10: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.ignore_errors` instead.\n"
          ]
        }
      ],
      "source": [
        "autotune = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.map(preprocess_data, num_parallel_calls=autotune)\n",
        "train_ds = train_ds.shuffle(8 * BATCH_SIZE)\n",
        "train_ds = train_ds.padded_batch(\n",
        "    batch_size=BATCH_SIZE, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "train_ds = train_ds.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "train_ds = train_ds.apply(tf.data.experimental.ignore_errors())\n",
        "train_ds = train_ds.prefetch(autotune)\n",
        "\n",
        "eval_ds = eval_ds.map(preprocess_data, num_parallel_calls=autotune)\n",
        "eval_ds = eval_ds.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "eval_ds = eval_ds.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
        "eval_ds = eval_ds.apply(tf.data.experimental.ignore_errors())\n",
        "eval_ds = eval_ds.prefetch(autotune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iFntoT_-5mX",
        "outputId": "2b3ae95d-5b61-4967-855c-49e1695e54a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "     75/Unknown - 70s 472ms/step - loss: 4.1631\n",
            "Epoch 1: val_loss improved from inf to 4.04099, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_1\n",
            "75/75 [==============================] - 84s 669ms/step - loss: 4.1631 - val_loss: 4.0410\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 4.0690\n",
            "Epoch 2: val_loss improved from 4.04099 to 3.86982, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_2\n",
            "75/75 [==============================] - 53s 696ms/step - loss: 4.0690 - val_loss: 3.8698\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 3.4181\n",
            "Epoch 3: val_loss improved from 3.86982 to 3.11493, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_3\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 3.4181 - val_loss: 3.1149\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 3.1392\n",
            "Epoch 4: val_loss improved from 3.11493 to 2.91261, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_4\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 3.1392 - val_loss: 2.9126\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 2.2978\n",
            "Epoch 5: val_loss improved from 2.91261 to 1.85152, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_5\n",
            "75/75 [==============================] - 44s 566ms/step - loss: 2.2978 - val_loss: 1.8515\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.5535\n",
            "Epoch 6: val_loss improved from 1.85152 to 1.51841, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_6\n",
            "75/75 [==============================] - 44s 565ms/step - loss: 1.5535 - val_loss: 1.5184\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.3423\n",
            "Epoch 7: val_loss did not improve from 1.51841\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 1.3423 - val_loss: 1.5570\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.3019\n",
            "Epoch 8: val_loss did not improve from 1.51841\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 1.3019 - val_loss: 1.7247\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.4005\n",
            "Epoch 9: val_loss did not improve from 1.51841\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 1.4005 - val_loss: 1.5581\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.2177\n",
            "Epoch 10: val_loss improved from 1.51841 to 1.21522, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_10\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 1.2177 - val_loss: 1.2152\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.1628\n",
            "Epoch 11: val_loss improved from 1.21522 to 1.15262, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_11\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 1.1628 - val_loss: 1.1526\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.9934\n",
            "Epoch 12: val_loss improved from 1.15262 to 0.99283, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_12\n",
            "75/75 [==============================] - 44s 566ms/step - loss: 0.9934 - val_loss: 0.9928\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.7890\n",
            "Epoch 13: val_loss improved from 0.99283 to 0.98864, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_13\n",
            "75/75 [==============================] - 43s 563ms/step - loss: 0.7890 - val_loss: 0.9886\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 1.0677\n",
            "Epoch 14: val_loss did not improve from 0.98864\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 1.0677 - val_loss: 1.1004\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.8097\n",
            "Epoch 15: val_loss improved from 0.98864 to 0.82012, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_15\n",
            "75/75 [==============================] - 43s 562ms/step - loss: 0.8097 - val_loss: 0.8201\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.6520\n",
            "Epoch 16: val_loss improved from 0.82012 to 0.78852, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_16\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 0.6520 - val_loss: 0.7885\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.6309\n",
            "Epoch 17: val_loss improved from 0.78852 to 0.78003, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_17\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 0.6309 - val_loss: 0.7800\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5741\n",
            "Epoch 18: val_loss improved from 0.78003 to 0.70678, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_18\n",
            "75/75 [==============================] - 43s 565ms/step - loss: 0.5741 - val_loss: 0.7068\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5633\n",
            "Epoch 19: val_loss did not improve from 0.70678\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.5633 - val_loss: 0.7144\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5418\n",
            "Epoch 20: val_loss did not improve from 0.70678\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.5418 - val_loss: 0.7079\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5334\n",
            "Epoch 21: val_loss did not improve from 0.70678\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.5334 - val_loss: 0.7770\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5194\n",
            "Epoch 22: val_loss improved from 0.70678 to 0.68172, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_22\n",
            "75/75 [==============================] - 44s 566ms/step - loss: 0.5194 - val_loss: 0.6817\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5101\n",
            "Epoch 23: val_loss improved from 0.68172 to 0.67361, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_23\n",
            "75/75 [==============================] - 44s 565ms/step - loss: 0.5101 - val_loss: 0.6736\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4812\n",
            "Epoch 24: val_loss did not improve from 0.67361\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.4812 - val_loss: 0.6898\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4626\n",
            "Epoch 25: val_loss did not improve from 0.67361\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.4626 - val_loss: 0.7486\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4496\n",
            "Epoch 26: val_loss did not improve from 0.67361\n",
            "75/75 [==============================] - 42s 545ms/step - loss: 0.4496 - val_loss: 0.7011\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4437\n",
            "Epoch 27: val_loss did not improve from 0.67361\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.4437 - val_loss: 0.6874\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4215\n",
            "Epoch 28: val_loss improved from 0.67361 to 0.62272, saving model to /content/drive/Shareddrives/seai_project/models/prova/weights_epoch_28\n",
            "75/75 [==============================] - 43s 564ms/step - loss: 0.4215 - val_loss: 0.6227\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4151\n",
            "Epoch 29: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.4151 - val_loss: 0.6718\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4070\n",
            "Epoch 30: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.4070 - val_loss: 0.6906\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4145\n",
            "Epoch 31: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.4145 - val_loss: 0.6535\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.4236\n",
            "Epoch 32: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.4236 - val_loss: 0.6502\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3837\n",
            "Epoch 33: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3837 - val_loss: 0.6840\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3738\n",
            "Epoch 34: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.3738 - val_loss: 0.6476\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3759\n",
            "Epoch 35: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3759 - val_loss: 0.6679\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3714\n",
            "Epoch 36: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3714 - val_loss: 0.6459\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3494\n",
            "Epoch 37: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3494 - val_loss: 0.6732\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3455\n",
            "Epoch 38: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3455 - val_loss: 0.6609\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3393\n",
            "Epoch 39: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3393 - val_loss: 0.6715\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3317\n",
            "Epoch 40: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 546ms/step - loss: 0.3317 - val_loss: 0.6967\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3235\n",
            "Epoch 41: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.3235 - val_loss: 0.6984\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3115\n",
            "Epoch 42: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.3115 - val_loss: 0.6802\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3138\n",
            "Epoch 43: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 549ms/step - loss: 0.3138 - val_loss: 0.6314\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2973\n",
            "Epoch 44: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.2973 - val_loss: 0.6807\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3055\n",
            "Epoch 45: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.3055 - val_loss: 0.6358\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2952\n",
            "Epoch 46: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.2952 - val_loss: 0.6837\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2744\n",
            "Epoch 47: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.2744 - val_loss: 0.7182\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2647\n",
            "Epoch 48: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 549ms/step - loss: 0.2647 - val_loss: 0.6983\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2737\n",
            "Epoch 49: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 548ms/step - loss: 0.2737 - val_loss: 0.7295\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2569\n",
            "Epoch 50: val_loss did not improve from 0.62272\n",
            "75/75 [==============================] - 42s 547ms/step - loss: 0.2569 - val_loss: 0.6950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a7677790940>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=eval_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQNAR_veLrJl"
      },
      "source": [
        "#Inference *Float32*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnve7-b5vj2O",
        "outputId": "723a2d17-3432-407f-d6a9-a4e19c846467"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x796d2c672e60>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "model.load_weights(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "A7AsQZ-2vlY8"
      },
      "outputs": [],
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.4)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "vu511BRfvm-T"
      },
      "outputs": [],
      "source": [
        "iou_list = []\n",
        "\n",
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "eval_ds = tf.data.TFRecordDataset('/content/drive/Shareddrives/seai_project/test.record')\n",
        "eval_ds = eval_ds.map(parse_tfrecord)\n",
        "width = 1360\n",
        "height = 800\n",
        "\n",
        "for sample in eval_ds:\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_image(image)\n",
        "    detections = inference_model.predict(input_image, verbose = 0)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [(int(x)) for x in detections.nmsed_classes[0][:num_detections]]\n",
        "\n",
        "    pred = (detections.nmsed_boxes[0][:num_detections] / ratio)\n",
        "    normalized_pred = pred / tf.constant([width, height, width, height], dtype=tf.float32)\n",
        "    iou = compute_iou(normalized_pred, sample[\"objects\"][\"bbox\"])\n",
        "    iou_list.extend(iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG_41bNq5yXt",
        "outputId": "4dd94cf0-5190-4a27-dc6b-2c033a33e58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average IOU: 0.78816456\n"
          ]
        }
      ],
      "source": [
        "means = [tf.reduce_mean(arr).numpy() for arr in iou_list]\n",
        "global_mean = tf.reduce_mean(means).numpy()\n",
        "\n",
        "print(\"Average IOU:\", global_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTUJY5eno1jK"
      },
      "source": [
        "#Inference float16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/Shareddrives/seai_project/models/prova'\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "loss_fn = RetinaNetLoss(NUM_CLASSES)"
      ],
      "metadata": {
        "id": "yfHkT9qNmu9h"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_backbone = get_backbone()\n",
        "input_shape = (None, None, None, 3)\n",
        "old_model = RetinaNet(NUM_CLASSES, resnet50_backbone)\n",
        "old_model.build(input_shape)\n",
        "old_model.compile(loss=loss_fn, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "1JSUjEaHh3DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b645a8d0-f3b2-4da5-bc61-45aa066069d0"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "UWFqKru1u7WQ"
      },
      "outputs": [],
      "source": [
        "resnet50_backbone_float16 = get_backbone_float16()\n",
        "\n",
        "model = RetinaNetFloat16(NUM_CLASSES, resnet50_backbone_float16)\n",
        "model.build(input_shape)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "_KDlnJJ1sjk-"
      },
      "outputs": [],
      "source": [
        "latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "old_model.load_weights(latest_checkpoint)\n",
        "model.set_weights(old_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (None, None, None, 3)\n",
        "model.build(input_shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VujtTj3-8rrq",
        "outputId": "1400f47d-b3a0-49fe-9250-6f57cd19c3dc"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"RetinaNetFloat16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " FeaturePyramidFloat16 (Fea  multiple                  31585152  \n",
            " turePyramidFloat16)                                             \n",
            "                                                                 \n",
            " sequential_66 (Sequential)  (None, None, None, 387)   3252355   \n",
            "                                                                 \n",
            " sequential_67 (Sequential)  (None, None, None, 36)    2443300   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37280807 (71.31 MB)\n",
            "Trainable params: 37227687 (71.11 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "GS4XanpRs9aY"
      },
      "outputs": [],
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.4)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iou_list = []\n",
        "\n",
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image_float16(image, jitter=None)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    image = tf.cast(image, tf.float16)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "eval_ds = tf.data.TFRecordDataset('/content/drive/Shareddrives/seai_project/test.record')\n",
        "eval_ds = eval_ds.map(parse_tfrecord)\n",
        "width = 1360\n",
        "height = 800\n",
        "\n",
        "for sample in eval_ds:\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_image(image)\n",
        "    detections = inference_model.predict(input_image, verbose = 0)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "\n",
        "    pred = (detections.nmsed_boxes[0][:num_detections] / ratio)\n",
        "    pred = tf.cast(pred, dtype=tf.float16)\n",
        "\n",
        "    normalized_pred = pred / tf.constant([width, height, width, height], dtype=tf.float16)\n",
        "    iou = compute_iou(normalized_pred, tf.cast(sample[\"objects\"][\"bbox\"], tf.float16))\n",
        "    iou_list.extend(iou)"
      ],
      "metadata": {
        "id": "OQbckIsWUzsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means = [tf.reduce_mean(arr).numpy() for arr in iou_list]\n",
        "global_mean = tf.reduce_mean(means).numpy()\n",
        "\n",
        "print(\"Average IOU:\", global_mean)"
      ],
      "metadata": {
        "id": "oysLx3uKU3Hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6a0079-f483-4e3b-de34-22ccb50e03fc"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average IOU: 0.7876\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LqeLdZUc-rOm",
        "BXPVJVRDCocW",
        "Om8SlKsPRECz",
        "LoK6gNCx-vDa",
        "kczIbubjARZ7",
        "OvxMr0BXtlq_",
        "wTUJY5eno1jK"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}